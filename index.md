# Experimentation of Image-To-Image Translation

Welcome! So you're interested computer vision-- where do you start? **Image-to-image translation** is a specific class of vision and graphics problems that can help introduce you into the field. First, let's define what image-to-image translation is: given images in some source domain A, the goal is to map the image into some target domain B while still retaining the content representations. It's analagous to how we might use technology like Google Translate to translate sentences from English to Spanish so that they have the same meaning. Effective deep learning models can do this translation much quicker and more efficiently than the average person. 

There are several applications of image-to-image translation due to its applicability to multiple domains.  Existing methods include those for 

- aerial photography to maps; 
- daytime to nighttime photography; 
- sketch outlines to colored images; 
- superresolution;
- etc. 

There are also a lot of different tools and techniques that can be used to implement these applications, the most common of which are **Generative Adversarial Networks (GANs)**. To give a basic explanation, GANs are a neural network that typically contain a generator model to create new synthetic images in the target domain and a discriminator model that returns a probability of how likely it is that the image is real or fake. By directly competing with one another, both models improve so that the generator is able to learn how to create more and more realistic images and the discriminator improves its ability to discern whether an image is real during testing. 

> **NOTE:** 
> 
> There are many different types of GANs that exist-- for example, **Pix2Pix** is a specific type of GAN that was designed for general image-to-image translation purposes. Pix2Pix uses **paired datasets** for training where there are images in the source domain that correspond directly to images in the target domain. 
> 
> **CycleGANs** are also a type of GAN, but differ in that they can use **unpaired datasets**-- aka datasets that have images in both domains that do not have matches in the other-- by using *two* generators and *two* discriminators. To train these models, the CycleGAN essentially generates an image from source domain A to target domain B, then generates a new image in domain A based on the image from domain B. One of the discriminators will then compare how this new image in domain A compares to the original, allowing the generators to improve. The CycleGAN will also take images from domain B, translate to domain A, then translate again to domain B and compare with another discriminator to ensure that both generators (and discriminators) are being trained properly to create realistic images. 

This repository contains code, documentation, dataset, and results for a Pix2Pix model that translates black-and-white images to color and a CycleGAN model that attempts to do a style transfer so that older movies are updated with modern day effects. The source code for both models is credited to [Jun-Yan Zhu](https://github.com/junyanz)  and [Taesung Park](https://github.com/taesungp) and was supported by [Tongzhou Wang](https://github.com/SsnL). Our team-- Andrew Farabow, Aditi Diwan, and Nivi Ramalingam-- updated their work with datasets and code that we used to run experiments on both models that can be used to further understand how GANs work. The results can also be found within the repository. 

## Table of Contents
Last add clickable links; optional

## Getting Started
Before working with the code, ensure that the following pre-requisites are met:
-  Linux or macOS
-  Python 3
-  CPU or NVIDIA GPU + CUDA CuDNN

To clone this repo (after ensuring that your SSH key is set up), use the commands
```
git clone git@github.com:AndrewAF1/cs4664-pix2pix.git
git submodule update --init
```
Install the required dependencies-- these can also be found in the file [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1)/requirements.txt
- torch>=1.4.0
- torchvision>=0.5.0
- dominate>=2.4.0
- visdom>=0.1.8.8
- wandb

Now you are ready to begin working with your models!

## Black-and-White to Color
Description, using Pix2Pix

### Data Pre-Processing
Subdirectory structure, explain the data we used

### Training
Commands for training

### Experiment 1: Running Open-Source Implementation with NCD

### Experiment 2: Hyperparameter Optimization

### Experiment 3: Comparison of Generator/Discriminator Architectures

### Experiment 4: Comparison with Baseline Model(s)

### Experiment 5: Assessment of Performance on Landscapes Dataset

### Experiment 6: Resiliency of Model (on Noisy Data)

### Experiment 7: Addition of Unsupervised Regularization Involving Data Augmentation

## Special FX Style Transfer

### Data Pre-Processing

### Training

### Experiment 1: Running Open-Source Implementation 

### Experiment 2: Tuning Hyperparameters

## Common Issues and Advice
TODO Talk about long training times, issues with path, sort of like FAQ

## References
Our doc? 





